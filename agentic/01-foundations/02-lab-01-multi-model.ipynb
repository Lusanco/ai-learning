{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from typing import cast\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Type hinting\n",
    "from openai.types.chat import ChatCompletionUserMessageParam\n",
    "\n",
    "# Markdown output display\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to String\n",
    "def env_to_str(env: str) -> str:\n",
    "    return cast(str, os.getenv(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ollama Environment Variables\n",
    "ollama_api_key = env_to_str('OLLAMA_API_KEY')\n",
    "ollama_base_url = env_to_str('OLLAMA_BASE_URL')\n",
    "\n",
    "# Ollama AI Models\n",
    "model_deepseek_r1 = env_to_str('MODEL_DEEPSEEK_R1')\n",
    "model_qwen3  = env_to_str('MODEL_QWEN3')\n",
    "model_llama3 = env_to_str('MODEL_LLAMA3')\n",
    "model_gemma3 = env_to_str('MODEL_GEMMA3')\n",
    "\n",
    "# Gemini Environment Variables\n",
    "gemini_api_key = env_to_str('GEMINI_API_KEY')\n",
    "gemini_base_url = env_to_str('GEMINI_BASE_URL')\n",
    "\n",
    "# Gemini AI Models\n",
    "model_gemini_flash = env_to_str('MODEL_GEMINI_FLASH')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Environment Variables\n",
    "def check_api_key(api_key: str, ai_client: str):\n",
    "    if api_key:\n",
    "        print(f\"{ai_client} API Key exists and begins with: {api_key[:3]}...\")\n",
    "    else:\n",
    "        print(f\"{ai_client} API Key not set. Please check the .env file.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking keys for ollama and gemini api\n",
    "check_api_key(ollama_api_key, \"Ollama\")\n",
    "check_api_key(gemini_api_key, \"Gemini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Instances OpenAI Class\n",
    "ollama = OpenAI(api_key=ollama_api_key, base_url=ollama_base_url)\n",
    "gemini = OpenAI(api_key=gemini_api_key, base_url=gemini_base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Model w/type hinting line of code included\n",
    "def ask_model(prompt: str, ai_client: OpenAI, ai_model: str) -> str:\n",
    "    \n",
    "    # Create a list of messages in the OpenAI format\n",
    "    messages: list[ChatCompletionUserMessageParam] # type hinting\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = ai_client.chat.completions.create(\n",
    "        model=ai_model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # Display and return the result\n",
    "    result: str = cast(str, response.choices[0].message.content)\n",
    "    display(Markdown(result))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 01\n",
    "prompt = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.\"\n",
    "\n",
    "# Deepseek R1 - Formulating the Question\n",
    "question = ask_model(prompt, ollama, model_deepseek_r1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competidors = []\n",
    "answers = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 02\n",
    "\n",
    "# Qwen3's Answer\n",
    "competidors.append(model_qwen3)\n",
    "answer = ask_model(question, ollama, model_qwen3)\n",
    "answers.append(answer)\n",
    "\n",
    "# Llama3's Answer\n",
    "competidors.append(model_llama3)\n",
    "answer = ask_model(question, ollama, model_llama3)\n",
    "answers.append(answer)\n",
    "\n",
    "# Gemma3's Answer\n",
    "competidors.append(model_gemma3)\n",
    "answer = ask_model(question, ollama, model_gemma3)\n",
    "answers.append(answer)\n",
    "\n",
    "# Gemini 2.0 Flash's Answer\n",
    "competidors.append(model_llama3)\n",
    "answer = ask_model(question, gemini, model_gemini_flash)\n",
    "answers.append(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
