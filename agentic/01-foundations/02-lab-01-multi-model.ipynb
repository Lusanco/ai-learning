{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from typing import cast\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Type hinting\n",
    "from openai.types.chat import ChatCompletionUserMessageParam\n",
    "from openai.types.shared.chat_model import ChatModel\n",
    "\n",
    "# Markdown output display\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ollama Environment Variables\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "ollama_base_url = os.getenv('OLLAMA_BASE_URL')\n",
    "ollama_deepseek_r1: str = cast(str, os.getenv('OLLAMA_DEEPSEEK_R1'))\n",
    "ollama_qwen3: str = cast(str, os.getenv('OLLAMA_QWEN3'))\n",
    "ollama_llama3: str = cast(str, os.getenv('OLLAMA_LLAMA3'))\n",
    "ollama_gemma3: str = cast(str, os.getenv('OLLAMA_GEMMA3'))\n",
    "\n",
    "# Gemini Environment Variables\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "gemini_base_url = os.getenv('GEMINI_BASE_URL')\n",
    "gemini_flash: str = cast(str, os.getenv('GEMINI_FLASH'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Environment Variables\n",
    "def check_api_key(api_key: str | None, ai_model: str | None):\n",
    "    if api_key:\n",
    "        print(f\"{ai_model} API Key exists and begins with: {api_key[:3]}...\")\n",
    "    else:\n",
    "        print(f\"{ai_model} API Key not set. Please check the .env file.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_api_key(ollama_api_key, ollama_deepseek_r1)\n",
    "check_api_key(ollama_api_key, ollama_qwen3)\n",
    "check_api_key(ollama_api_key, ollama_llama3)\n",
    "check_api_key(ollama_api_key, ollama_gemma3)\n",
    "check_api_key(gemini_api_key, gemini_flash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Instances OpenAI Class\n",
    "ollama_client = OpenAI(api_key=ollama_api_key, base_url=ollama_base_url)\n",
    "gemini_client = OpenAI(api_key=gemini_api_key, base_url=gemini_base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Model w/type hinting line of code included\n",
    "def ask_model(prompt: str, ai_client: OpenAI, ai_model: str) -> str | None:\n",
    "    \n",
    "    # Create a list of messages in the OpenAI format\n",
    "    messages: list[ChatCompletionUserMessageParam] # type hinting\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = ai_client.chat.completions.create(\n",
    "        model=ai_model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # Display and return the result\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 01\n",
    "prompt = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.\"\n",
    "result = ask_model(prompt, ollama_client, ollama_deepseek_r1)\n",
    "ask_model(prompt, ollama_client, ollama_qwen3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 02\n",
    "prompt = f\"What is a critical pain point in that an Agentic AI could resolve for the following?\\n\\n{result}\"\n",
    "result = ask_model(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 03\n",
    "prompt = f\"Propose an Agentic AI-driven solution for: {result}.\"\n",
    "result = ask_model(prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
