{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86da83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import asyncio\n",
    "from typing import cast\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict\n",
    "from agents import Agent, Runner, OpenAIChatCompletionsModel, AsyncOpenAI\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "\n",
    "\n",
    "# Arize Phoenix Tracer\n",
    "from phoenix.otel import register\n",
    "from opentelemetry import trace as otel_trace\n",
    "\n",
    "\n",
    "# Markdown output display\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178bbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to String\n",
    "def env_to_str(env: str) -> str:\n",
    "    return cast(str, os.getenv(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057f3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "\n",
    "# Load Environment Variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ollama Environment Variables\n",
    "ollama_api_key = env_to_str('OLLAMA_API_KEY')\n",
    "ollama_base_url = env_to_str('OLLAMA_BASE_URL')\n",
    "\n",
    "# Gemini Environment Variables\n",
    "gemini_api_key = env_to_str(\"GEMINI_API_KEY\")\n",
    "gemini_base_url = env_to_str(\"GEMINI_BASE_URL\")\n",
    "\n",
    "# Phoenix Collector Variables\n",
    "phoenix_collector_endpoint = env_to_str(\"PHOENIX_COLLECTOR_ENDPOINT\") or \"http://localhost:6006/v1/traces\"\n",
    "\n",
    "# Maileroo Environment Variables\n",
    "maileroo_api_key = env_to_str(\"MAILEROO_API_KEY\")\n",
    "maileroo_base_url = env_to_str(\"MAILEROO_BASE_URL\")\n",
    "maileroo_template_url = env_to_str(\"MAILEROO_TEMPLATE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2799d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available AI Models\n",
    "\n",
    "# Ollama General Models\n",
    "model_mistral = env_to_str(\"MODEL_MISTRAL\")\n",
    "model_llama = env_to_str('MODEL_LLAMA3')\n",
    "model_mistral_nemo = env_to_str(\"MODEL_MISTRAL_NEMO\")\n",
    "model_gemma = env_to_str(\"MODEL_GEMMA3\")\n",
    "\n",
    "# Ollama Resoning Models \n",
    "model_phi = env_to_str(\"MODEL_PHI3.5\")\n",
    "model_qwen = env_to_str(\"MODEL_QWEN3\")\n",
    "model_deepseek = env_to_str(\"MODEL_DEEPSEEK_R1\")\n",
    "\n",
    "# Gemini Models\n",
    "model_gemini_flash = env_to_str(\"MODEL_GEMINI_FLASH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a multi model setup. The current model is: llama3.1:8b\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "\n",
    "# Set Current Model\n",
    "current_model = model_llama\n",
    "\n",
    "# Multi Model Flag\n",
    "multi_model = False\n",
    "\n",
    "# Check for the need of multiple models, \n",
    "# and if not, make sure to have selected \n",
    "# a current ai model.\n",
    "if multi_model and current_model != \"\":\n",
    "    print(\"This is a multi model setup. Leave current model as empty string: ''.\")\n",
    "elif multi_model and current_model == \"\":\n",
    "    print(\"This is a multi model setup. \")\n",
    "elif not multi_model and current_model == \"\":\n",
    "    print(\"This is a single model setup. Please set a current model.\")\n",
    "elif not multi_model and current_model != \"\":\n",
    "    print(f\"Not a multi model setup. The current model is: {model_llama}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8457437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Workflow Instructions\n",
    "\n",
    "instructions1 = \"You are a sales agent working for ComplAI, \\\n",
    "a company that provides a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI. \\\n",
    "You write professional, serious cold emails.\"\n",
    "\n",
    "instructions2 = \"You are a humorous, engaging sales agent working for ComplAI, \\\n",
    "a company that provides a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI. \\\n",
    "You write witty, engaging cold emails that are likely to get a response.\"\n",
    "\n",
    "instructions3 = \"You are a busy sales agent working for ComplAI, \\\n",
    "a company that provides a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI. \\\n",
    "You write concise, to the point cold emails.\"\n",
    "\n",
    "instructions4 = \"You pick the best cold sales email from the given options. \\\n",
    "Imagine you are a customer and pick the one you are most likely to respond to. \\\n",
    "Do not give an explanation; reply with the selected email only.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f720087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DependencyConflict: requested: \"openai-agents >= 0.1.0\" but found: \"openai-agents 0.0.17\"\n",
      "DependencyConflict: requested: \"google-genai\" but found: \"None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: ollama_phoenix\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: http://localhost:6006/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "‚úÖ Phoenix tracer registered successfully"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure Phoenix Tracer\n",
    "try:\n",
    "    tracer_provider = register(\n",
    "    project_name=\"ollama_phoenix\",\n",
    "    auto_instrument=True,\n",
    "    endpoint=phoenix_collector_endpoint,\n",
    "    set_global_tracer_provider=True\n",
    ")\n",
    "    display(Markdown(\"‚úÖ Phoenix tracer registered successfully\"))\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"‚ùå Phoenix registration failed: {e}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a7488b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama Model\n",
    "ollama_model = OpenAIChatCompletionsModel(\n",
    "    model=model_llama,\n",
    "    openai_client=AsyncOpenAI(\n",
    "        api_key=ollama_api_key, \n",
    "        base_url=ollama_base_url\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6aec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents\n",
    "\n",
    "sales_agent1 = Agent(\n",
    "        name=\"Professional Sales Agent\",\n",
    "        instructions=instructions1,\n",
    "        model=ollama_model\n",
    ")\n",
    "\n",
    "sales_agent2 = Agent(\n",
    "        name=\"Engaging Sales Agent\",\n",
    "        instructions=instructions2,\n",
    "        model=ollama_model\n",
    ")\n",
    "\n",
    "sales_agent3 = Agent(\n",
    "        name=\"Busy Sales Agent\",\n",
    "        instructions=instructions3,\n",
    "        model=ollama_model\n",
    ")\n",
    "\n",
    "sales_picker = Agent(\n",
    "    name=\"sales_picker\",\n",
    "    instructions=instructions4,\n",
    "    model=ollama_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b875b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phoenix Tracer\n",
    "async def phoenix_tracer(trace_name, message, *agents):\n",
    "    tracer = otel_trace.get_tracer(__name__)\n",
    "    with tracer.start_as_current_span(trace_name) as current_span:\n",
    "        if agents:\n",
    "            \n",
    "            # Trace Attributes\n",
    "            current_span.set_attribute(\"user.request\", message)\n",
    "            current_span.set_attribute(\"model.name\", model_llama)\n",
    "            \n",
    "            for index, agent in enumerate(agents):\n",
    "                \n",
    "                # Trace Attributes\n",
    "                current_span.set_attribute(\"agent.name\", agent.name)\n",
    "        \n",
    "                results = await asyncio.gather(\n",
    "                    *[Runner.run(agent, message) for agent in agents]\n",
    "                )\n",
    "\n",
    "                outputs = []\n",
    "\n",
    "                for result in results:\n",
    "                    outputs.append(result.final_output)\n",
    "\n",
    "                combined_outputs = \"\\n\\n--- Option ---\\n\\n\".join(outputs)\n",
    "                best = await Runner.run(agents[-1], combined_outputs)\n",
    "\n",
    "                if outputs:\n",
    "                    total_length = sum(len(output) for output in outputs)\n",
    "                    \n",
    "                    # Add response attributes\n",
    "                    current_span.set_attribute(\"response.length\", total_length)\n",
    "                    # current_span.set_attribute(\"response.preview\", outputs[0][:100] if outputs[0] else \"\")\n",
    "                    current_span.set_attribute(\"response.preview\", outputs[0] if outputs[0] else \"\")\n",
    "        else:\n",
    "            print(\"No additional arguments provided.\")\n",
    "\n",
    "        # outputs = [result.final_output for result in results]\n",
    "        \n",
    "        \n",
    "        for output in outputs:\n",
    "            display(Markdown(f\"# Sales Agent:\\n\\n{output}\\n\\n\"))\n",
    "            display(Markdown(f\"# Sales Picker:\\n\\n{best}\"))\n",
    "        return outputs, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062cd323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: ollama_o*********ault. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n",
      "Exception while exporting Span.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n",
      "    yield span\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/openinference/instrumentation/_tracers.py\", line 141, in start_as_current_span\n",
      "    yield cast(OpenInferenceSpan, current_span)\n",
      "  File \"/var/folders/mz/sc9xd4w571bctk3y250vd0dm0000gn/T/ipykernel_5970/2744680573.py\", line 16, in phoenix_tracer\n",
      "    results = await asyncio.gather(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/run.py\", line 219, in run\n",
      "    input_guardrail_results, turn_result = await asyncio.gather(\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/run.py\", line 787, in _run_single_turn\n",
      "    new_response = await cls._get_new_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/run.py\", line 946, in _get_new_response\n",
      "    new_response = await model.get_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py\", line 62, in get_response\n",
      "    response = await self._fetch_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py\", line 264, in _fetch_response\n",
      "    ret = await self._get_client().chat.completions.create(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1748, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1490, in request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 136, in handle_async_request\n",
      "    raise exc\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 106, in handle_async_request\n",
      "    ) = await self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 177, in _receive_response_headers\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 217, in _receive_event\n",
      "    data = await self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 35, in read\n",
      "    return await self._stream.receive(max_bytes=max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 1254, in receive\n",
      "    await self._protocol.read_event.wait()\n",
      "  File \"/Users/luissantiago/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/locks.py\", line 212, in wait\n",
      "    await fut\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connection.py\", line 445, in request\n",
      "    self.endheaders()\n",
      "  File \"/Users/luissantiago/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Users/luissantiago/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Users/luissantiago/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connection.py\", line 276, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x144221b50>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=6006): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x144221b50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 140, in _export\n",
      "    resp = self._session.post(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=6006): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x144221b50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connection.py\", line 445, in request\n",
      "    self.endheaders()\n",
      "  File \"/Users/luissantiago/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Users/luissantiago/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Users/luissantiago/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connection.py\", line 276, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x144511df0>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=6006): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x144511df0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/export/__init__.py\", line 109, in on_end\n",
      "    self.span_exporter.export((span,))\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 165, in export\n",
      "    resp = self._export(serialized_data, deadline_sec - time())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 148, in _export\n",
      "    resp = self._session.post(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luissantiago/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/requests/adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=6006): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x144511df0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Result\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result, best = \u001b[38;5;28;01mawait\u001b[39;00m phoenix_tracer(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mParallel Cold Emails\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWrite a cold sales email.\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      5\u001b[39m     sales_agent1, \n\u001b[32m      6\u001b[39m     sales_agent2, \n\u001b[32m      7\u001b[39m     sales_agent3,\n\u001b[32m      8\u001b[39m     sales_picker\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mphoenix_tracer\u001b[39m\u001b[34m(trace_name, message, *agents)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agents):\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Trace Attributes\u001b[39;00m\n\u001b[32m     14\u001b[39m     current_span.set_attribute(\u001b[33m\"\u001b[39m\u001b[33magent.name\u001b[39m\u001b[33m\"\u001b[39m, agent.name)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m     17\u001b[39m         *[Runner.run(agent, message) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents]\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     20\u001b[39m     outputs = []\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/run.py:219\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[39m\n\u001b[32m    214\u001b[39m logger.debug(\n\u001b[32m    215\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    216\u001b[39m )\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    220\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    221\u001b[39m             starting_agent,\n\u001b[32m    222\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    223\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    224\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    225\u001b[39m             context_wrapper,\n\u001b[32m    226\u001b[39m         ),\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    228\u001b[39m             agent=current_agent,\n\u001b[32m    229\u001b[39m             all_tools=all_tools,\n\u001b[32m    230\u001b[39m             original_input=original_input,\n\u001b[32m    231\u001b[39m             generated_items=generated_items,\n\u001b[32m    232\u001b[39m             hooks=hooks,\n\u001b[32m    233\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    234\u001b[39m             run_config=run_config,\n\u001b[32m    235\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    236\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    237\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    238\u001b[39m         ),\n\u001b[32m    239\u001b[39m     )\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    241\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    242\u001b[39m         agent=current_agent,\n\u001b[32m    243\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    251\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    252\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/run.py:787\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    785\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    788\u001b[39m     agent,\n\u001b[32m    789\u001b[39m     system_prompt,\n\u001b[32m    790\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    791\u001b[39m     output_schema,\n\u001b[32m    792\u001b[39m     all_tools,\n\u001b[32m    793\u001b[39m     handoffs,\n\u001b[32m    794\u001b[39m     context_wrapper,\n\u001b[32m    795\u001b[39m     run_config,\n\u001b[32m    796\u001b[39m     tool_use_tracker,\n\u001b[32m    797\u001b[39m     previous_response_id,\n\u001b[32m    798\u001b[39m )\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    801\u001b[39m     agent=agent,\n\u001b[32m    802\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    812\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/run.py:946\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    943\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m    944\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    947\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    948\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    949\u001b[39m     model_settings=model_settings,\n\u001b[32m    950\u001b[39m     tools=all_tools,\n\u001b[32m    951\u001b[39m     output_schema=output_schema,\n\u001b[32m    952\u001b[39m     handoffs=handoffs,\n\u001b[32m    953\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    954\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    955\u001b[39m     ),\n\u001b[32m    956\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    957\u001b[39m )\n\u001b[32m    959\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py:62\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     48\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m     previous_response_id: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     56\u001b[39m ) -> ModelResponse:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     58\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     59\u001b[39m         model_config=model_settings.to_json_dict() | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m     60\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     61\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     63\u001b[39m             system_instructions,\n\u001b[32m     64\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     65\u001b[39m             model_settings,\n\u001b[32m     66\u001b[39m             tools,\n\u001b[32m     67\u001b[39m             output_schema,\n\u001b[32m     68\u001b[39m             handoffs,\n\u001b[32m     69\u001b[39m             span_generation,\n\u001b[32m     70\u001b[39m             tracing,\n\u001b[32m     71\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     72\u001b[39m         )\n\u001b[32m     74\u001b[39m         first_choice = response.choices[\u001b[32m0\u001b[39m]\n\u001b[32m     75\u001b[39m         message = first_choice.message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py:264\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream)\u001b[39m\n\u001b[32m    258\u001b[39m store = ChatCmplHelpers.get_store_param(\u001b[38;5;28mself\u001b[39m._get_client(), model_settings)\n\u001b[32m    260\u001b[39m stream_options = ChatCmplHelpers.get_stream_options_param(\n\u001b[32m    261\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    262\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    265\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    266\u001b[39m     messages=converted_messages,\n\u001b[32m    267\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    268\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    269\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    270\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.frequency_penalty),\n\u001b[32m    271\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.presence_penalty),\n\u001b[32m    272\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    273\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    274\u001b[39m     response_format=response_format,\n\u001b[32m    275\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    276\u001b[39m     stream=stream,\n\u001b[32m    277\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(stream_options),\n\u001b[32m    278\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(store),\n\u001b[32m    279\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(reasoning_effort),\n\u001b[32m    280\u001b[39m     extra_headers={**HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    281\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    282\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    283\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    284\u001b[39m )\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1985\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1986\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1987\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2025\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2026\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2027\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m             {\n\u001b[32m   2032\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m             },\n\u001b[32m   2064\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m         ),\n\u001b[32m   2068\u001b[39m         options=make_request_options(\n\u001b[32m   2069\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m         ),\n\u001b[32m   2071\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/openai/_base_client.py:1748\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1734\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1735\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1736\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1743\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1744\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1745\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1746\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1747\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/openai/_base_client.py:1490\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1488\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1490\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1491\u001b[39m         request,\n\u001b[32m   1492\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1493\u001b[39m         **kwargs,\n\u001b[32m   1494\u001b[39m     )\n\u001b[32m   1495\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1496\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai-learning/agentic/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1254\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1249\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1252\u001b[39m ):\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: ollama_o*********ault. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "result, best = await phoenix_tracer(\n",
    "    \"Parallel Cold Emails\", \n",
    "    \"Write a cold sales email.\", \n",
    "    sales_agent1, \n",
    "    sales_agent2, \n",
    "    sales_agent3,\n",
    "    sales_picker\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
